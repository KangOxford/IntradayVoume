{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/Users/kang/Untitled-100.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/kang/Desktop/2017/20170705.data', 'r') as file:\n",
    "    data = file.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gunzip /Users/kang/Desktop/2017/20170705.csv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gunzip /Users/kang/Desktop/2017/20170706.csv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ticker</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>OPCL</th>\n",
       "      <th>pvCLCL</th>\n",
       "      <th>prevAdjClose</th>\n",
       "      <th>SPpvCLCL</th>\n",
       "      <th>sharesOut</th>\n",
       "      <th>PERMNO</th>\n",
       "      <th>SICCD</th>\n",
       "      <th>PERMCO</th>\n",
       "      <th>prevRawOpen</th>\n",
       "      <th>prevRawClose</th>\n",
       "      <th>prevAdjOpen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>SPY</td>\n",
       "      <td>242.63</td>\n",
       "      <td>243.00999</td>\n",
       "      <td>241.700</td>\n",
       "      <td>242.77</td>\n",
       "      <td>54427596</td>\n",
       "      <td>0.000577</td>\n",
       "      <td>0.002312</td>\n",
       "      <td>242.21</td>\n",
       "      <td>0.001453</td>\n",
       "      <td>978982</td>\n",
       "      <td>84398</td>\n",
       "      <td>6726</td>\n",
       "      <td>46699</td>\n",
       "      <td>242.88</td>\n",
       "      <td>242.21001</td>\n",
       "      <td>242.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>IWM</td>\n",
       "      <td>141.70</td>\n",
       "      <td>141.85001</td>\n",
       "      <td>140.700</td>\n",
       "      <td>141.59</td>\n",
       "      <td>21865451</td>\n",
       "      <td>-0.000777</td>\n",
       "      <td>-0.003589</td>\n",
       "      <td>142.10</td>\n",
       "      <td>0.001453</td>\n",
       "      <td>267550</td>\n",
       "      <td>88222</td>\n",
       "      <td>6726</td>\n",
       "      <td>37493</td>\n",
       "      <td>141.34</td>\n",
       "      <td>142.10001</td>\n",
       "      <td>141.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>EEM</td>\n",
       "      <td>41.39</td>\n",
       "      <td>41.57000</td>\n",
       "      <td>41.185</td>\n",
       "      <td>41.55</td>\n",
       "      <td>49490955</td>\n",
       "      <td>0.003858</td>\n",
       "      <td>-0.001922</td>\n",
       "      <td>41.63</td>\n",
       "      <td>0.001453</td>\n",
       "      <td>780300</td>\n",
       "      <td>89730</td>\n",
       "      <td>6726</td>\n",
       "      <td>37493</td>\n",
       "      <td>41.62</td>\n",
       "      <td>41.63000</td>\n",
       "      <td>41.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>TLT</td>\n",
       "      <td>124.20</td>\n",
       "      <td>124.65000</td>\n",
       "      <td>124.115</td>\n",
       "      <td>124.49</td>\n",
       "      <td>11394834</td>\n",
       "      <td>0.002332</td>\n",
       "      <td>0.000241</td>\n",
       "      <td>124.46</td>\n",
       "      <td>0.001453</td>\n",
       "      <td>60500</td>\n",
       "      <td>89468</td>\n",
       "      <td>6726</td>\n",
       "      <td>37493</td>\n",
       "      <td>125.00</td>\n",
       "      <td>124.46000</td>\n",
       "      <td>125.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>USO</td>\n",
       "      <td>9.56</td>\n",
       "      <td>9.58000</td>\n",
       "      <td>9.230</td>\n",
       "      <td>9.25</td>\n",
       "      <td>68181979</td>\n",
       "      <td>-0.032964</td>\n",
       "      <td>-0.038462</td>\n",
       "      <td>9.62</td>\n",
       "      <td>0.001453</td>\n",
       "      <td>346100</td>\n",
       "      <td>91208</td>\n",
       "      <td>6221</td>\n",
       "      <td>50617</td>\n",
       "      <td>9.55</td>\n",
       "      <td>9.62000</td>\n",
       "      <td>9.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2924</th>\n",
       "      <td>2925</td>\n",
       "      <td>ZTO</td>\n",
       "      <td>14.15</td>\n",
       "      <td>14.88000</td>\n",
       "      <td>14.150</td>\n",
       "      <td>14.67</td>\n",
       "      <td>2667647</td>\n",
       "      <td>0.036090</td>\n",
       "      <td>0.036017</td>\n",
       "      <td>14.16</td>\n",
       "      <td>0.001453</td>\n",
       "      <td>525306</td>\n",
       "      <td>16449</td>\n",
       "      <td>4215</td>\n",
       "      <td>55781</td>\n",
       "      <td>14.00</td>\n",
       "      <td>14.16000</td>\n",
       "      <td>14.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2925</th>\n",
       "      <td>2926</td>\n",
       "      <td>ZTR</td>\n",
       "      <td>12.80</td>\n",
       "      <td>12.81000</td>\n",
       "      <td>12.720</td>\n",
       "      <td>12.72</td>\n",
       "      <td>116837</td>\n",
       "      <td>-0.006270</td>\n",
       "      <td>0.003946</td>\n",
       "      <td>12.67</td>\n",
       "      <td>0.001453</td>\n",
       "      <td>25673</td>\n",
       "      <td>75367</td>\n",
       "      <td>6726</td>\n",
       "      <td>21955</td>\n",
       "      <td>12.72</td>\n",
       "      <td>12.67000</td>\n",
       "      <td>12.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2926</th>\n",
       "      <td>2927</td>\n",
       "      <td>ZTS</td>\n",
       "      <td>62.71</td>\n",
       "      <td>62.88000</td>\n",
       "      <td>62.335</td>\n",
       "      <td>62.55</td>\n",
       "      <td>3000331</td>\n",
       "      <td>-0.002555</td>\n",
       "      <td>-0.001915</td>\n",
       "      <td>62.67</td>\n",
       "      <td>0.001453</td>\n",
       "      <td>490797</td>\n",
       "      <td>13788</td>\n",
       "      <td>2834</td>\n",
       "      <td>54327</td>\n",
       "      <td>62.40</td>\n",
       "      <td>62.67000</td>\n",
       "      <td>62.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2927</th>\n",
       "      <td>2928</td>\n",
       "      <td>ZX</td>\n",
       "      <td>1.65</td>\n",
       "      <td>1.83000</td>\n",
       "      <td>1.600</td>\n",
       "      <td>1.64</td>\n",
       "      <td>49708</td>\n",
       "      <td>-0.006079</td>\n",
       "      <td>-0.012048</td>\n",
       "      <td>1.66</td>\n",
       "      <td>0.001453</td>\n",
       "      <td>206440</td>\n",
       "      <td>12720</td>\n",
       "      <td>3312</td>\n",
       "      <td>53748</td>\n",
       "      <td>1.81</td>\n",
       "      <td>1.66000</td>\n",
       "      <td>1.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2928</th>\n",
       "      <td>2929</td>\n",
       "      <td>ZYME</td>\n",
       "      <td>8.28</td>\n",
       "      <td>8.60690</td>\n",
       "      <td>8.280</td>\n",
       "      <td>8.48</td>\n",
       "      <td>6831</td>\n",
       "      <td>0.023867</td>\n",
       "      <td>0.024155</td>\n",
       "      <td>8.28</td>\n",
       "      <td>0.001453</td>\n",
       "      <td>24940</td>\n",
       "      <td>16668</td>\n",
       "      <td>8731</td>\n",
       "      <td>55912</td>\n",
       "      <td>8.40</td>\n",
       "      <td>8.28000</td>\n",
       "      <td>8.40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2929 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0 ticker    open       high      low   close    volume  \\\n",
       "0              1    SPY  242.63  243.00999  241.700  242.77  54427596   \n",
       "1              2    IWM  141.70  141.85001  140.700  141.59  21865451   \n",
       "2              3    EEM   41.39   41.57000   41.185   41.55  49490955   \n",
       "3              4    TLT  124.20  124.65000  124.115  124.49  11394834   \n",
       "4              5    USO    9.56    9.58000    9.230    9.25  68181979   \n",
       "...          ...    ...     ...        ...      ...     ...       ...   \n",
       "2924        2925    ZTO   14.15   14.88000   14.150   14.67   2667647   \n",
       "2925        2926    ZTR   12.80   12.81000   12.720   12.72    116837   \n",
       "2926        2927    ZTS   62.71   62.88000   62.335   62.55   3000331   \n",
       "2927        2928     ZX    1.65    1.83000    1.600    1.64     49708   \n",
       "2928        2929   ZYME    8.28    8.60690    8.280    8.48      6831   \n",
       "\n",
       "          OPCL    pvCLCL  prevAdjClose  SPpvCLCL  sharesOut  PERMNO SICCD  \\\n",
       "0     0.000577  0.002312        242.21  0.001453     978982   84398  6726   \n",
       "1    -0.000777 -0.003589        142.10  0.001453     267550   88222  6726   \n",
       "2     0.003858 -0.001922         41.63  0.001453     780300   89730  6726   \n",
       "3     0.002332  0.000241        124.46  0.001453      60500   89468  6726   \n",
       "4    -0.032964 -0.038462          9.62  0.001453     346100   91208  6221   \n",
       "...        ...       ...           ...       ...        ...     ...   ...   \n",
       "2924  0.036090  0.036017         14.16  0.001453     525306   16449  4215   \n",
       "2925 -0.006270  0.003946         12.67  0.001453      25673   75367  6726   \n",
       "2926 -0.002555 -0.001915         62.67  0.001453     490797   13788  2834   \n",
       "2927 -0.006079 -0.012048          1.66  0.001453     206440   12720  3312   \n",
       "2928  0.023867  0.024155          8.28  0.001453      24940   16668  8731   \n",
       "\n",
       "      PERMCO  prevRawOpen  prevRawClose  prevAdjOpen  \n",
       "0      46699       242.88     242.21001       242.88  \n",
       "1      37493       141.34     142.10001       141.34  \n",
       "2      37493        41.62      41.63000        41.62  \n",
       "3      37493       125.00     124.46000       125.00  \n",
       "4      50617         9.55       9.62000         9.55  \n",
       "...      ...          ...           ...          ...  \n",
       "2924   55781        14.00      14.16000        14.00  \n",
       "2925   21955        12.72      12.67000        12.72  \n",
       "2926   54327        62.40      62.67000        62.40  \n",
       "2927   53748         1.81       1.66000         1.82  \n",
       "2928   55912         8.40       8.28000         8.40  \n",
       "\n",
       "[2929 rows x 18 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file into a pandas DataFrame\n",
    "data = pd.read_csv(\"/Users/kang/Desktop/2017/20170705.csv\")\n",
    "\n",
    "# Now `data` is a DataFrame containing the data from the CSV file\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file into a pandas DataFrame\n",
    "data6 = pd.read_csv('/Users/kang/Desktop/2017/20170706.csv')\n",
    "data5 = pd.read_csv('/Users/kang/Desktop/2017/20170705.csv')\n",
    "\n",
    "# Now `data` is a DataFrame containing the data from the CSV file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ticker</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>OPCL</th>\n",
       "      <th>pvCLCL</th>\n",
       "      <th>prevAdjClose</th>\n",
       "      <th>SPpvCLCL</th>\n",
       "      <th>sharesOut</th>\n",
       "      <th>PERMNO</th>\n",
       "      <th>SICCD</th>\n",
       "      <th>PERMCO</th>\n",
       "      <th>prevRawOpen</th>\n",
       "      <th>prevRawClose</th>\n",
       "      <th>prevAdjOpen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Unnamed: 0, ticker, open, high, low, close, volume, OPCL, pvCLCL, prevAdjClose, SPpvCLCL, sharesOut, PERMNO, SICCD, PERMCO, prevRawOpen, prevRawClose, prevAdjOpen]\n",
       "Index: []"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data.ticker=='A']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/wm/hz1dg7s12290bq4v0lkczywc0000gn/T/ipykernel_31745/391604064.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mihai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"~/AAP_2017-07-05_24900000_57900000_message_10.csv\",header=None)\n",
    "df.columns = ['time','type','price','qty','orderid','side','remark']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>type</th>\n",
       "      <th>price</th>\n",
       "      <th>qty</th>\n",
       "      <th>orderid</th>\n",
       "      <th>side</th>\n",
       "      <th>remark</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25200.236312</td>\n",
       "      <td>1</td>\n",
       "      <td>2518417</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>MZHO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25200.236315</td>\n",
       "      <td>1</td>\n",
       "      <td>2518421</td>\n",
       "      <td>100</td>\n",
       "      <td>1999999900</td>\n",
       "      <td>-1</td>\n",
       "      <td>MZHO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28805.201858</td>\n",
       "      <td>1</td>\n",
       "      <td>3635385</td>\n",
       "      <td>25</td>\n",
       "      <td>1300000</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28805.209164</td>\n",
       "      <td>1</td>\n",
       "      <td>3635393</td>\n",
       "      <td>10</td>\n",
       "      <td>1859500</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28805.213655</td>\n",
       "      <td>1</td>\n",
       "      <td>3635405</td>\n",
       "      <td>75</td>\n",
       "      <td>1950000</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93014</th>\n",
       "      <td>57662.536905</td>\n",
       "      <td>1</td>\n",
       "      <td>227171057</td>\n",
       "      <td>50</td>\n",
       "      <td>1300000</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93015</th>\n",
       "      <td>57721.216336</td>\n",
       "      <td>1</td>\n",
       "      <td>227214801</td>\n",
       "      <td>61</td>\n",
       "      <td>1266600</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93016</th>\n",
       "      <td>57721.242143</td>\n",
       "      <td>1</td>\n",
       "      <td>227215117</td>\n",
       "      <td>18</td>\n",
       "      <td>965400</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93017</th>\n",
       "      <td>57797.200428</td>\n",
       "      <td>1</td>\n",
       "      <td>227250057</td>\n",
       "      <td>1</td>\n",
       "      <td>1053400</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93018</th>\n",
       "      <td>57853.573340</td>\n",
       "      <td>1</td>\n",
       "      <td>227279789</td>\n",
       "      <td>200</td>\n",
       "      <td>1050100</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>93019 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               time  type      price  qty     orderid  side remark\n",
       "0      25200.236312     1    2518417  100         100     1   MZHO\n",
       "1      25200.236315     1    2518421  100  1999999900    -1   MZHO\n",
       "2      28805.201858     1    3635385   25     1300000    -1    NaN\n",
       "3      28805.209164     1    3635393   10     1859500    -1    NaN\n",
       "4      28805.213655     1    3635405   75     1950000    -1    NaN\n",
       "...             ...   ...        ...  ...         ...   ...    ...\n",
       "93014  57662.536905     1  227171057   50     1300000    -1    NaN\n",
       "93015  57721.216336     1  227214801   61     1266600    -1    NaN\n",
       "93016  57721.242143     1  227215117   18      965400     1    NaN\n",
       "93017  57797.200428     1  227250057    1     1053400    -1    NaN\n",
       "93018  57853.573340     1  227279789  200     1050100     1    NaN\n",
       "\n",
       "[93019 rows x 7 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/wm/hz1dg7s12290bq4v0lkczywc0000gn/T/ipykernel_31745/3952749887.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/x/lib/python3.9/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__nonzero__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1525\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__nonzero__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mNoReturn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1527\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m   1528\u001b[0m             \u001b[0;34mf\"The truth value of a {type(self).__name__} is ambiguous. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m             \u001b[0;34m\"Use a.empty, a.bool(), a.item(), a.any() or a.all().\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all()."
     ]
    }
   ],
   "source": [
    "df[(df.type==3) or (df.type==4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/wm/hz1dg7s12290bq4v0lkczywc0000gn/T/ipykernel_31745/2285947374.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m3\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/x/lib/python3.9/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__nonzero__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1525\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__nonzero__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mNoReturn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1527\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m   1528\u001b[0m             \u001b[0;34mf\"The truth value of a {type(self).__name__} is ambiguous. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m             \u001b[0;34m\"Use a.empty, a.bool(), a.item(), a.any() or a.all().\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all()."
     ]
    }
   ],
   "source": [
    "df[df.type==3 in (3,4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4174946"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.type==1].qty.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "533858"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.type==4].qty.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "275108"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.type==5].qty.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "720"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.type==6].qty.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.type==7].qty.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "808966"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.type==4].qty.sum()+df[df.type==5].qty.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "809686"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.type==4].qty.sum()+df[df.type==5].qty.sum()+df[df.type==6].qty.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4984632"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.type==4].qty.sum()+df[df.type==5].qty.sum()+df[df.type==1].qty.sum()+df[df.type==6].qty.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7705769"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.type==1].qty.sum()+df[df.type==3].qty.sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "808966"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.type==4].qty.sum()+df[df.type==5].qty.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The volume in the open high low close volume dataset from Yahoo Finance represents the total number of shares traded in a security period\n",
    "\n",
    "s the number of shares of the stock that were traded on that day. It is an important metric for investors to consider, as it can give insights into the liquidity of the stock and the level of interest in it from buyers and sellers.\n",
    "\n",
    "```python\n",
    "import yfinance as yf\n",
    "data = yf.download(\"AAP\", start=start_date, end=end_date)\n",
    "data[\"Volume\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7,348,683"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting yfinance\n",
      "  Using cached yfinance-0.2.31-py2.py3-none-any.whl (65 kB)\n",
      "Requirement already satisfied: pandas>=1.3.0 in ./opt/anaconda3/envs/x/lib/python3.9/site-packages (from yfinance) (1.5.1)\n",
      "Requirement already satisfied: numpy>=1.16.5 in ./opt/anaconda3/envs/x/lib/python3.9/site-packages (from yfinance) (1.23.4)\n",
      "Collecting requests>=2.31 (from yfinance)\n",
      "  Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "Collecting multitasking>=0.0.7 (from yfinance)\n",
      "  Using cached multitasking-0.0.11-py3-none-any.whl (8.5 kB)\n",
      "Requirement already satisfied: lxml>=4.9.1 in ./opt/anaconda3/envs/x/lib/python3.9/site-packages (from yfinance) (4.9.2)\n",
      "Requirement already satisfied: appdirs>=1.4.4 in ./opt/anaconda3/envs/x/lib/python3.9/site-packages (from yfinance) (1.4.4)\n",
      "Collecting pytz>=2022.5 (from yfinance)\n",
      "  Using cached pytz-2023.3.post1-py2.py3-none-any.whl (502 kB)\n",
      "Collecting frozendict>=2.3.4 (from yfinance)\n",
      "  Using cached frozendict-2.3.8-cp39-cp39-macosx_11_0_arm64.whl (35 kB)\n",
      "Collecting peewee>=3.16.2 (from yfinance)\n",
      "  Using cached peewee-3.17.0-cp39-cp39-macosx_11_0_arm64.whl\n",
      "Requirement already satisfied: beautifulsoup4>=4.11.1 in ./opt/anaconda3/envs/x/lib/python3.9/site-packages (from yfinance) (4.11.1)\n",
      "Collecting html5lib>=1.1 (from yfinance)\n",
      "  Using cached html5lib-1.1-py2.py3-none-any.whl (112 kB)\n",
      "Requirement already satisfied: soupsieve>1.2 in ./opt/anaconda3/envs/x/lib/python3.9/site-packages (from beautifulsoup4>=4.11.1->yfinance) (2.3.2.post1)\n",
      "Requirement already satisfied: six>=1.9 in ./opt/anaconda3/envs/x/lib/python3.9/site-packages (from html5lib>=1.1->yfinance) (1.16.0)\n",
      "Requirement already satisfied: webencodings in ./opt/anaconda3/envs/x/lib/python3.9/site-packages (from html5lib>=1.1->yfinance) (0.5.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in ./opt/anaconda3/envs/x/lib/python3.9/site-packages (from pandas>=1.3.0->yfinance) (2.8.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./opt/anaconda3/envs/x/lib/python3.9/site-packages (from requests>=2.31->yfinance) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./opt/anaconda3/envs/x/lib/python3.9/site-packages (from requests>=2.31->yfinance) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./opt/anaconda3/envs/x/lib/python3.9/site-packages (from requests>=2.31->yfinance) (1.26.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./opt/anaconda3/envs/x/lib/python3.9/site-packages (from requests>=2.31->yfinance) (2022.12.7)\n",
      "Installing collected packages: pytz, peewee, multitasking, requests, html5lib, frozendict, yfinance\n",
      "  Attempting uninstall: pytz\n",
      "    Found existing installation: pytz 2022.1\n",
      "    Uninstalling pytz-2022.1:\n",
      "      Successfully uninstalled pytz-2022.1\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.26.0\n",
      "    Uninstalling requests-2.26.0:\n",
      "      Successfully uninstalled requests-2.26.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "spyder 5.3.3 requires pyqt5<5.16, which is not installed.\n",
      "spyder 5.3.3 requires pyqtwebengine<5.16, which is not installed.\n",
      "stable-baselines3 1.7.0 requires gym==0.21, but you have gym 0.26.2 which is incompatible.\n",
      "stable-baselines3 1.7.0 requires importlib-metadata~=4.13, but you have importlib-metadata 4.11.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed frozendict-2.3.8 html5lib-1.1 multitasking-0.0.11 peewee-3.17.0 pytz-2023.3.post1 requests-2.28.2 yfinance-0.2.31\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install yfinance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to get ticker 'AAP' reason: Expecting value: line 1 column 1 (char 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1 Failed download:\n",
      "['AAP']: Exception('%ticker%: No timezone found, symbol may be delisted')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Series([], Name: Volume, dtype: float64)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "data = yf.download(\"AAP\", start=\"20170701\", end=\"20170710\")\n",
    "data[\"Volume\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\tMessage File:\t\t(Matrix of size: (Nx6))\n",
    "\t-------------\t\n",
    "\t\t\t\n",
    "\tName: \tTICKER_Year-Month-Day_StartTime_EndTime_message_LEVEL.csv \t\n",
    "\t\t\n",
    "\t\tStartTime and EndTime give the theoretical beginning \n",
    "\t\tand end time of the output file in milliseconds after \t\t\n",
    "\t\tmid night. LEVEL refers to the number of levels of the \n",
    "\t\trequested limit order book.\n",
    "\n",
    "\n",
    "\tColumns:\n",
    "\t\n",
    "\t    1.) Time: \t\t\n",
    "\t\t\t\tSeconds after midnight with decimal \n",
    "\t\t\t\tprecision of at least milliseconds \n",
    "\t\t\t\tand up to nanoseconds depending on \n",
    "\t\t\t\tthe requested period\n",
    "\t    2.) Type:\n",
    "\t\t\t\t1: Submission of a new limit order\n",
    "\t\t\t\t2: Cancellation (Partial deletion \n",
    "\t\t\t\t   of a limit order)\n",
    "\t\t\t\t3: Deletion (Total deletion of a limit order)\n",
    "\t\t\t\t4: Execution of a visible limit order\t\t\t   \t \n",
    "\t\t\t\t5: Execution of a hidden limit order\n",
    "\t\t\t\t6: Indicates a cross trade, e.g. auction trade\n",
    "\t\t\t\t7: Trading halt indicator \t\t\t\t   \n",
    "\t\t\t\t   (Detailed information below)\n",
    "\t    3.) Order ID: \t\n",
    "\t\t\t\tUnique order reference number \n",
    "\t\t\t\t(Assigned in order flow)\n",
    "\t    4.) Size: \t\t\n",
    "\t\t\t\tNumber of shares\n",
    "\t    5.) Price: \t\t\n",
    "\t\t\t\tDollar price times 10000 \n",
    "\t\t\t\t(i.e., A stock price of $91.14 is given \n",
    "\t\t\t\tby 911400)\n",
    "\t    6.) Direction:\n",
    "\t\t\t\t-1: Sell limit order\n",
    "\t\t\t\t1: Buy limit order\n",
    "\t\t\t\t\n",
    "\t\t\t\tNote: \n",
    "\t\t\t\tExecution of a sell (buy) limit\n",
    "\t\t\t\torder corresponds to a buyer (seller) \n",
    "\t\t\t\tinitiated trade, i.e. Buy (Sell) trade.\n",
    "\t\t\t\t\t\t\t\t\t\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test r2 results from the R output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "dir0 = \"/homes/80/kang/cmem/output/0400_r_kl_output_raw_data_notional/\"\n",
    "onlyFiles = sorted([f for f in listdir(dir0) if isfile(join(dir0, f))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "dir0 = \"/homes/80/kang/cmem/output/0400_r_kl_output_raw_data_notional/\"\n",
    "onlyFiles = sorted([f for f in listdir(dir0) if isfile(join(dir0, f))])\n",
    "dfflst =[]\n",
    "for i in range(len(onlyFiles)):\n",
    "    print(i)\n",
    "    name = onlyFiles[i]\n",
    "    import pandas as pd\n",
    "    # name =\"CMS.csv\"\n",
    "\n",
    "    dir = dir0+name\n",
    "    df = pd.read_csv(dir)\n",
    "    df[\"date\"]=df.date.apply(lambda x: x[-5:])\n",
    "    from sklearn.metrics import r2_score\n",
    "    lst = []\n",
    "    g=df.groupby(\"date\")\n",
    "    for idx, itm in g:\n",
    "        r2=r2_score(itm[\"original\"],itm[\"forecast_signal\"])\n",
    "        lst.append([idx,name[:-4],r2])\n",
    "    dff=pd.DataFrame(lst,columns=[\"date\",\"symbol\",\"r2\"])\n",
    "    dfflst.append(dff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.concat(dfflst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = pd.pivot(d,index=\"date\",columns=\"symbol\",values=\"r2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.reset_option('display.max_rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for itm in f.mean(axis=0):\n",
    "    print(itm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for itm in f.mean(axis=1):\n",
    "    print(itm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.mean(axis=1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.mean(axis=0).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "dir0 = \"/homes/80/kang/2017daily_csv/\"\n",
    "onlyFiles = sorted([f for f in listdir(dir0) if isfile(join(dir0, f))])\n",
    "dfflst =[]\n",
    "i=0\n",
    "# for i in range(len(onlyFiles)):\n",
    "print(i)\n",
    "name = onlyFiles[i]\n",
    "import pandas as pd\n",
    "# name =\"CMS.csv\"\n",
    "\n",
    "dir = dir0+name\n",
    "df = pd.read_csv(dir)\n",
    "df.ticker.to_list()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "dir0 = \"/homes/80/kang/cmem/data/01.1_raw/\"\n",
    "dir1 = \"/homes/80/kang/2017daily_csv/\"\n",
    "onlyFiles = sorted([f for f in listdir(dir0) if isfile(join(dir0, f))])\n",
    "\n",
    "i=0\n",
    "# for i in range(len(onlyFiles)):\n",
    "print(i)\n",
    "name = onlyFiles[i]\n",
    "import pandas as pd\n",
    "# name =\"CMS.csv\"\n",
    "\n",
    "df0 = pd.read_pickle(dir0+name)\n",
    "df1 = pd.read_csv(dir1+name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# merge the shareOut data and convert into symbol "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import pandas as pd\n",
    "dir0 = \"/homes/80/kang/cmem/data/01.1_raw/\"\n",
    "dir1 = \"/homes/80/kang/2017daily_csv/\"\n",
    "onlyFiles = sorted([f for f in listdir(dir1) if isfile(join(dir1, f))])\n",
    "\n",
    "i=0\n",
    "lst =[]\n",
    "# for i in range(len(onlyFiles)):\n",
    "print(i)\n",
    "name = onlyFiles[i]\n",
    "df1 = pd.read_csv(dir1+name)\n",
    "d1=df1[['date',\"ticker\", \"volume\", \"sharesOut\"]]\n",
    "lst.append(d1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import pandas as pd\n",
    "dir0 = \"/homes/80/kang/cmem/data/01.1_raw/\"\n",
    "dir1 = \"/homes/80/kang/2017daily_csv/\"\n",
    "onlyFiles = sorted([f for f in listdir(dir1) if isfile(join(dir1, f))])\n",
    "\n",
    "i=0\n",
    "lst =[]\n",
    "for i in range(len(onlyFiles)):\n",
    "    print(i)\n",
    "    name = onlyFiles[i]\n",
    "    df1 = pd.read_csv(dir1+name)\n",
    "    df1['date']=name[:-4]\n",
    "    d1=df1[['date',\"ticker\", \"volume\", \"sharesOut\"]]\n",
    "    lst.append(d1)\n",
    "f=pd.concat(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir /homes/80/kang/2017daily_aggregate/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.to_csv(\"/homes/80/kang/2017daily_aggregate/2017daily_aggregate.csv\")\n",
    "f.to_pickle(\"/homes/80/kang/2017daily_aggregate/2017daily_aggregate.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=pd.read_pickle(\"/homes/80/kang/2017daily_aggregate/2017daily_aggregate.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g=f[[ \"date\", \"ticker\", \"sharesOut\"]]\n",
    "h=g.pivot(columns=\"ticker\",index=\"date\",values=\"sharesOut\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h.to_csv(\"/homes/80/kang/2017daily_aggregate/2017sharesOut.csv\")\n",
    "h.to_pickle(\"/homes/80/kang/2017daily_aggregate/2017sharesOut.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the data and prepare for merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h=pd.read_pickle(\"/homes/80/kang/2017daily_aggregate/2017sharesOut.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/homes/80/kang/cmem/stock_names.txt\") as f:\n",
    "    lst=[itm.strip() for itm in f.readlines()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count =0\n",
    "for itm in lst:\n",
    "    if itm in df.ticker.to_list():\n",
    "        count+=1\n",
    "        print(itm)\n",
    "# 384"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = h[h.columns.intersection(lst)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(h.columns.tolist()) == sorted(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h=h[sorted(h.columns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l=h.dropna(axis=1)\n",
    "# Out[33]: \n",
    "# ticker         AAL      AAP       ABBV       ABC     ABMD        ABT       ACN       ADI  ...        WY      XEC       XEL      XLB       XLE        XLF       XLI       XLK\n",
    "# date                                                                                      ...                                                                               \n",
    "# 20170103  507294.0  73654.0  1625099.0  220100.0  43508.0  1472310.0  622630.0  308171.0  ...  748058.0  94964.0  507953.0  73974.0  234074.0   971695.0  163576.0  293606.0\n",
    "# 20170104  507294.0  73654.0  1625099.0  220100.0  43508.0  1472310.0  622630.0  308171.0  ...  748058.0  94964.0  507953.0  73974.0  234074.0   971695.0  163576.0  293606.0\n",
    "# 20170105  507294.0  73654.0  1625099.0  220100.0  43508.0  1472310.0  622630.0  308171.0  ...  748058.0  94964.0  507953.0  73974.0  234074.0   971695.0  163576.0  293606.0\n",
    "# 20170106  507294.0  73654.0  1625099.0  220100.0  43508.0  1472310.0  622630.0  308171.0  ...  748058.0  94964.0  507953.0  73974.0  234074.0   971695.0  163576.0  293606.0\n",
    "# 20170109  507294.0  73654.0  1625099.0  220100.0  43508.0  1472310.0  622630.0  308171.0  ...  748058.0  94964.0  507953.0  73974.0  234074.0   971695.0  163576.0  293606.0\n",
    "# ...            ...      ...        ...       ...      ...        ...       ...       ...  ...       ...      ...       ...      ...       ...        ...       ...       ...\n",
    "# 20171222  478499.0  73898.0  1596430.0  218082.0  44216.0  1740601.0  614718.0  368636.0  ...  754829.0  95261.0  507763.0  75674.0  239824.0  1138345.0  174376.0  305106.0\n",
    "# 20171226  478499.0  73898.0  1596430.0  218082.0  44216.0  1740601.0  614718.0  368636.0  ...  754829.0  95261.0  507763.0  75674.0  239824.0  1138345.0  174376.0  305106.0\n",
    "# 20171227  478499.0  73898.0  1596430.0  218082.0  44216.0  1740601.0  614718.0  368636.0  ...  754829.0  95261.0  507763.0  75674.0  239824.0  1138345.0  174376.0  305106.0\n",
    "# 20171228  478499.0  73898.0  1596430.0  218082.0  44216.0  1740601.0  614718.0  368636.0  ...  754829.0  95261.0  507763.0  75674.0  239824.0  1138345.0  174376.0  305106.0\n",
    "# 20171229  475508.0  73898.0  1596430.0  218082.0  44272.0  1740601.0  642550.0  368636.0  ...  754829.0  95261.0  507763.0  81274.0  250524.0  1170545.0  165776.0  302156.0\n",
    "\n",
    "# [251 rows x 384 columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l.to_csv(\"/homes/80/kang/2017daily_aggregate/2017sharesOutCommon.csv\")\n",
    "l.to_pickle(\"/homes/80/kang/2017daily_aggregate/2017sharesOutCommon.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deal with the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read the data with sharesOut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l=pd.read_pickle(\"/homes/80/kang/2017daily_aggregate/2017sharesOutCommon.pkl\")\n",
    "l.index = pd.to_datetime(l.index)\n",
    "date_filter = pd.Timestamp(\"2017-07-05\")\n",
    "l = l.loc[l.index >= date_filter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir /homes/80/kang/cmem/data/01.1_raw_fraction/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd /homes/80/kang/cmem/data/01.1_raw_fraction/\n",
    "rm *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir /homes/80/kang/cmem/data_fraction/01.1_raw_fraction/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "dir0 = \"/homes/80/kang/cmem/data/01.1_raw/\"\n",
    "dir1 = \"/homes/80/kang/cmem/data_fraction/01.1_raw_fraction/\"\n",
    "onlyFiles = sorted([f for f in listdir(dir0) if isfile(join(dir0, f))])\n",
    "\n",
    "i=100\n",
    "for i in range(len(onlyFiles)):\n",
    "    print(i)\n",
    "    name = onlyFiles[i]\n",
    "    symbol = name[:-4]\n",
    "    import pandas as pd\n",
    "    # name =\"CMS.csv\"\n",
    "\n",
    "    if symbol in l.columns.to_list():\n",
    "        df = pd.read_pickle(dir0+name)\n",
    "        dff=df.copy()\n",
    "        dff.index = dff.index.strftime(\"%Y-%m-%d\")\n",
    "        df[\"sharesOut\"] = l.loc[dff.index,symbol].values\n",
    "        if df.sharesOut.isna().sum()!=df.shape[0]:\n",
    "            print(\"saved\")\n",
    "            df['qty_fraction']=df.qty/df.sharesOut\n",
    "            df.to_pickle(dir1+symbol+\".pkl\")\n",
    "            df.to_csv(dir1+symbol+\".csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ls | wc -l\n",
    "768/2=384"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# after getting the 01.1_raw, we need to convert into r_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mkdir /homes/80/kang/cmem/data_fraction/02.2_data_r_input_kf_fraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path11 = '/homes/80/kang/cmem/data_fraction/01.1_raw_fraction/'\n",
    "path12 = '/homes/80/kang/cmem/data_fraction/02.2_data_r_input_kf/'\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import listdir;from os.path import isfile,join\n",
    "files = lambda path: sorted([f for f in listdir(path) if isfile(join(path,f)) and f[-3:]=='pkl'])\n",
    "files11 = files(path11)\n",
    "from tqdm import tqdm\n",
    "for i in tqdm(range(len(files11))):\n",
    "    name = files11[i][:-4]\n",
    "    df = pd.read_pickle(path11+files11[i])\n",
    "    resampled_df = df[['date','timeHMs','qty_fraction']]\n",
    "\n",
    "    # Step 2: Pivot the DataFrame to get the desired format\n",
    "    pivot_df = resampled_df.pivot_table(index=resampled_df.index.time, \\\n",
    "        columns=resampled_df.index.date, values='qty_fraction')\n",
    "    # Step 3: Create a new DataFrame with formatted column names\n",
    "    new_columns = [col.strftime('%Y-%m-%d') for col in pivot_df.columns]\n",
    "    pivot_df.columns = new_columns\n",
    "\n",
    "    # Step 4: Add the 'AM' or 'PM' label to the index\n",
    "    pivot_df.index = [f'{t.strftime(\"%I:%M %p\")}' for t in pivot_df.index]\n",
    "\n",
    "    # Step 5: Save the DataFrame to a CSV file\n",
    "    pivot_df.to_csv(path12 + name +\".csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run r with /homes/80/kang/cmem/rkf.r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir /homes/80/kang/cmem/output/0400_r_kl_output_raw_data_fractional/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calc the R2 of the R Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd /homes/80/kang/cmem/output/0400_r_kl_output_raw_data_fractional/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "dir0 = \"/homes/80/kang/cmem/output/0400_r_kl_output_raw_data_fractional/\"\n",
    "onlyFiles = sorted([f for f in listdir(dir0) if isfile(join(dir0, f))])\n",
    "dfflst = []\n",
    "import pandas as pd\n",
    "for i in range(len(onlyFiles)):\n",
    "    print(i)\n",
    "    name = onlyFiles[i]\n",
    "    import pandas as pd\n",
    "    # name =\"CMS.csv\"\n",
    "\n",
    "    dir = dir0+name\n",
    "    df = pd.read_csv(dir)\n",
    "    df[\"date\"]=df.date.apply(lambda x: x[-5:])\n",
    "    from sklearn.metrics import r2_score\n",
    "    lst = []\n",
    "    g=df.groupby(\"date\")\n",
    "    for idx, itm in g:\n",
    "        r2=r2_score(itm[\"original\"],itm[\"forecast_signal\"])\n",
    "        lst.append([idx,name[:-4],r2])\n",
    "    dff=pd.DataFrame(lst,columns=[\"date\",\"symbol\",\"r2\"])\n",
    "    dfflst.append(dff)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=pd.concat(dfflst)\n",
    "h=f.pivot(index=\"date\",columns=\"symbol\",values=\"r2\")\n",
    "h['symbol_mean']=h.mean(axis=1)\n",
    "h.loc['date_mean']=h.mean(axis=0)\n",
    "h.to_csv(\"/homes/80/kang/cmem/0400_r_kl_output_fractional_r2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The pipeline for max and min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mkdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir /homes/80/kang/cmem/data_minmax\n",
    "mkdir /homes/80/kang/cmem/data_minmax/01.1_raw_minmax/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calc the minmax before converting into r_input csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "dir0 = \"/homes/80/kang/cmem/data/01.1_raw/\"\n",
    "dir1 = \"/homes/80/kang/cmem/data_minmax/01.1_raw_minmax/\"\n",
    "onlyFiles = sorted([f for f in listdir(dir0) if isfile(join(dir0, f))])\n",
    "\n",
    "i=100\n",
    "for i in range(len(onlyFiles)):\n",
    "    print(i)\n",
    "    name = onlyFiles[i]\n",
    "    symbol = name[:-4]\n",
    "    import pandas as pd\n",
    "    # name =\"CMS.csv\"\n",
    "\n",
    "\n",
    "    df = pd.read_pickle(dir0+name)\n",
    "    df[\"qty_minmax\"] = df.qty/df.qty.max()\n",
    "    # df[\"qty_minmax\"] = df.qty/(df.qty.max()-df.qty.min())\n",
    "    # df[\"qty_minmax\"] = (df.qty-df.qty.min())/(df.qty.max()-df.qty.min())\n",
    "    \n",
    "    df.to_pickle(dir1+symbol+\".pkl\")\n",
    "    df.to_csv(dir1+symbol+\".csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After getting the 01.1_raw, we need to convert into r_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mkdir /homes/80/kang/cmem/data_minmax/\n",
    "mkdir /homes/80/kang/cmem/data_minmax/02.2_data_r_input_kf_minmax/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path11 = '/homes/80/kang/cmem/data_minmax/01.1_raw_minmax/'\n",
    "path12 = '/homes/80/kang/cmem/data_minmax/02.2_data_r_input_kf_minmax/'\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import listdir;from os.path import isfile,join\n",
    "files = lambda path: sorted([f for f in listdir(path) if isfile(join(path,f)) and f[-3:]=='pkl'])\n",
    "files11 = files(path11)\n",
    "from tqdm import tqdm\n",
    "for i in tqdm(range(len(files11))):\n",
    "    name = files11[i][:-4]\n",
    "    df = pd.read_pickle(path11+files11[i])\n",
    "    resampled_df = df[['date','timeHMs','qty_minmax']]\n",
    "\n",
    "    # Step 2: Pivot the DataFrame to get the desired format\n",
    "    pivot_df = resampled_df.pivot_table(index=resampled_df.index.time, \\\n",
    "        columns=resampled_df.index.date, values='qty_minmax')\n",
    "    # Step 3: Create a new DataFrame with formatted column names\n",
    "    new_columns = [col.strftime('%Y-%m-%d') for col in pivot_df.columns]\n",
    "    pivot_df.columns = new_columns\n",
    "\n",
    "    # Step 4: Add the 'AM' or 'PM' label to the index\n",
    "    pivot_df.index = [f'{t.strftime(\"%I:%M %p\")}' for t in pivot_df.index]\n",
    "\n",
    "    # Step 5: Save the DataFrame to a CSV file\n",
    "    pivot_df.to_csv(path12 + name +\".csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run r with /homes/80/kang/cmem/rkf.r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir /homes/80/kang/cmem/output/0400_r_kl_output_raw_data_minmax/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rscript /homes/80/kang/cmem/rkf_fraction.r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calc the R2 of the R Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "dir0 = \"/homes/80/kang/cmem/output/0400_r_kl_output_raw_data_minmax/\"\n",
    "onlyFiles = sorted([f for f in listdir(dir0) if isfile(join(dir0, f))])\n",
    "dfflst = []\n",
    "import pandas as pd\n",
    "for i in range(len(onlyFiles)):\n",
    "    print(i)\n",
    "    name = onlyFiles[i]\n",
    "    import pandas as pd\n",
    "    # name =\"CMS.csv\"\n",
    "\n",
    "    dir = dir0+name\n",
    "    df = pd.read_csv(dir)\n",
    "    df[\"date\"]=df.date.apply(lambda x: x[-5:])\n",
    "    from sklearn.metrics import r2_score\n",
    "    lst = []\n",
    "    g=df.groupby(\"date\")\n",
    "    for idx, itm in g:\n",
    "        r2=r2_score(itm[\"original\"],itm[\"forecast_signal\"])\n",
    "        lst.append([idx,name[:-4],r2])\n",
    "    dff=pd.DataFrame(lst,columns=[\"date\",\"symbol\",\"r2\"])\n",
    "    dfflst.append(dff)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=pd.concat(dfflst)\n",
    "h=f.pivot(index=\"date\",columns=\"symbol\",values=\"r2\")\n",
    "l=h.copy()\n",
    "l['symbol_mean']=l.mean(axis=1)\n",
    "l.loc['date_mean']=l.mean(axis=0)\n",
    "l.to_csv(\"/homes/80/kang/cmem/0400_r_kl_output_minmax_r2.csv\")\n",
    "h.mean(axis=1).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test the raw cmem output r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "dir0 = \"/homes/80/kang/cmem/output/06_r_output_raw_csv/\"\n",
    "onlyFiles = sorted([f for f in listdir(dir0) if isfile(join(dir0, f))])\n",
    "dfflst = []\n",
    "import pandas as pd\n",
    "i=0\n",
    "for i in range(len(onlyFiles)):\n",
    "    print(i)\n",
    "    name = onlyFiles[i]\n",
    "    import pandas as pd\n",
    "    # name =\"CMS.csv\"\n",
    "\n",
    "    dir = dir0+name\n",
    "    df = pd.read_csv(dir)\n",
    "    df.date = df.date.apply(lambda x:str(int(x)))\n",
    "    from sklearn.metrics import r2_score\n",
    "    lst = []\n",
    "    g=df.groupby(\"date\")\n",
    "    for idx, itm in g:\n",
    "        pass\n",
    "        r2=r2_score(itm[\"turnover\"],itm[\"x\"])\n",
    "        # r2=r2_score(itm[\"original\"],itm[\"forecast_signal\"])\n",
    "        lst.append([idx,name[:-4],r2])\n",
    "    dff=pd.DataFrame(lst,columns=[\"date\",\"symbol\",\"r2\"])\n",
    "    dfflst.append(dff)\n",
    "\n",
    "f=pd.concat(dfflst)\n",
    "h=f.pivot(index=\"date\",columns=\"symbol\",values=\"r2\")\n",
    "l=h.copy()\n",
    "l['symbol_mean']=l.mean(axis=1)\n",
    "l.loc['date_mean']=l.mean(axis=0)\n",
    "# l.to_csv(\"/homes/80/kang/cmem/0400_r_kl_output_qty_r2.csv\")\n",
    "h.mean(axis=1).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2 = pd.read_csv(\"/homes/80/kang/cmem/0400_r_kl_output_minmax_r2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SNA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pd.set_option('display.max_columns', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.width', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sna=pd.read_csv(\"/homes/80/kang/cmem/output/06_r_output_raw_csv/ACN.csv\")\n",
    "# sna=pd.read_csv(\"/homes/80/kang/cmem/output/06_r_output_raw_csv/SNA.csv\")\n",
    "df = sna[[\"date\", \"turnover\", \"x\", \"eta\", \"seas\", \"mu\"]]\n",
    "df['x'] = df['x'].apply(lambda x: f'{x:f}') \n",
    "df['eta'] = df['eta'].apply(lambda x: f'{x:f}')\n",
    "df\n",
    "import numpy as np\n",
    "g=df.groupby(\"date\")\n",
    "lst=[]\n",
    "for idx,itm in g:\n",
    "    itm['sum_turnover']=itm.turnover.astype(np.float64).sum()\n",
    "    itm['sum_x']=itm.x.astype(np.float64).sum()\n",
    "    itm['sum_eta']=itm.eta.astype(np.float64).sum()\n",
    "    lst.append(itm)\n",
    "f=pd.concat(lst)\n",
    "f.sum_tunover=f.sum_turnover.apply(lambda x: \"{:,.2}\".format(x)) \n",
    "f.sum_x=f.sum_x.apply(lambda x: \"{:,.2}\".format(x)) \n",
    "f.sum_eta=f.sum_eta.apply(lambda x: \"{:,.2}\".format(x)) \n",
    "f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error comes from the eps?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Roll back to volume space "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read one test file from the r output of the fractional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "dir0 = \"/homes/80/kang/cmem/output/0400_r_kl_output_raw_data_fractional/\"\n",
    "onlyFiles = sorted([f for f in listdir(dir0) if isfile(join(dir0, f))])\n",
    "dfflst = []\n",
    "import pandas as pd\n",
    "i=0\n",
    "# for i in range(len(onlyFiles)):\n",
    "print(i)\n",
    "name = onlyFiles[i]\n",
    "symbol=name[:-4]\n",
    "import pandas as pd\n",
    "# name =\"CMS.csv\"\n",
    "\n",
    "dir = dir0+name\n",
    "df = pd.read_csv(dir)\n",
    "df[\"date\"]=df.date.apply(lambda x: x[-5:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load the sharesOut data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read the data with sharesOut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l=pd.read_pickle(\"/homes/80/kang/2017daily_aggregate/2017sharesOutCommon.pkl\")\n",
    "l.index = pd.to_datetime(l.index)\n",
    "date_filter = pd.Timestamp(\"2017-07-19\")\n",
    "l = l.loc[l.index >= date_filter]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### choose the common dates from the df of 0400_r_kl_output_raw_data_fractional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"date\"] = df.date.apply(lambda x: \"2017-\" + x)\n",
    "df['date'] = pd.to_datetime(df['date'], format='%Y-%m.%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### merge the shareOut data into the r output df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sharesOut']=l.loc[df.date,symbol].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## roll back to the shares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['shares_forecast_signal']=df.forecast_signal*df.sharesOut\n",
    "df['shares_original']=df.original*df.sharesOut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## calc r2 inside the df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "lst = []\n",
    "g=df.groupby(\"date\")\n",
    "for idx, itm in g:\n",
    "    r2=r2_score(itm[\"shares_original\"],itm[\"shares_forecast_signal\"])\n",
    "    lst.append([idx,r2])\n",
    "dff=pd.DataFrame(lst,columns=[\"date\",\"r2\"])\n",
    "dff['date'] = pd.to_datetime(dff['date'], format='%Y-%m.%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff=dff.set_index(\"date\")\n",
    "df['shares_r2']=dff.loc[df.date,:].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save the data into"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir /homes/80/kang/cmem/output/0400_r_kl_output_raw_data_fractional_rollback2shares/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir /homes/80/kang/cmem/output/06_r_output_raw_csv_fractional_shares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"/homes/80/kang/cmem/output/06_r_output_raw_csv_fractional_shares/\"+symbol+\".csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# all pipeline of fractional_shares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l=pd.read_pickle(\"/homes/80/kang/2017daily_aggregate/2017sharesOutCommon.pkl\")\n",
    "# l=pd.read_pickle(\"/homes/80/kang/2017daily_aggregate/2017sharesOutCommon.pkl\")\n",
    "l.index = pd.to_datetime(l.index)\n",
    "date_filter = pd.Timestamp(\"2017-07-19\")\n",
    "l = l.loc[l.index >= date_filter]\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "dir0 = \"/homes/80/kang/cmem/output/0400_r_kl_output_raw_data_fractional/\"\n",
    "onlyFiles = sorted([f for f in listdir(dir0) if isfile(join(dir0, f))])\n",
    "dfflst = []\n",
    "import pandas as pd\n",
    "# i=1\n",
    "for i in range(len(onlyFiles)):\n",
    "    print(i)\n",
    "    name = onlyFiles[i]\n",
    "    symbol=name[:-4]\n",
    "    import pandas as pd\n",
    "    # name =\"CMS.csv\"\n",
    "\n",
    "    dir = dir0+name\n",
    "    df = pd.read_csv(dir)\n",
    "    df[\"date\"]=df.date.apply(lambda x: x[-5:])\n",
    "\n",
    "    df[\"date\"] = df.date.apply(lambda x: \"2017-\" + x)\n",
    "    df['date'] = pd.to_datetime(df['date'], format='%Y-%m.%d')\n",
    "    df['sharesOut']=l.loc[df.date,symbol].values\n",
    "    df['shares_forecast_signal']=df.forecast_signal*df.sharesOut\n",
    "    df['shares_original']=df.original*df.sharesOut\n",
    "    from sklearn.metrics import r2_score\n",
    "    lst = []\n",
    "    g=df.groupby(\"date\")\n",
    "    for idx, itm in g:\n",
    "        r2=r2_score(itm[\"shares_original\"],itm[\"shares_forecast_signal\"])\n",
    "        lst.append([idx,r2])\n",
    "    dff=pd.DataFrame(lst,columns=[\"date\",\"r2\"])\n",
    "    dff['date'] = pd.to_datetime(dff['date'], format='%Y-%m.%d')\n",
    "    dff=dff.set_index(\"date\")\n",
    "    df['shares_r2']=dff.loc[df.date,:].values\n",
    "    df.to_csv(\"/homes/80/kang/cmem/output/06_r_output_raw_csv_fractional_shares/\"+symbol+\".csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read 0702"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dir0 = \"/homes/80/kang/cmem/output/0702_single/\"\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "onlyFiles = sorted([f for f in listdir(dir0) if isfile(join(dir0, f))])\n",
    "dfflst = []\n",
    "import pandas as pd\n",
    "i=0\n",
    "# for i in range(len(onlyFiles)):\n",
    "print(i)\n",
    "name = onlyFiles[i]\n",
    "symbol=name[:-4]\n",
    "import pandas as pd\n",
    "# name =\"CMS.csv\"\n",
    "\n",
    "dir = dir0+name\n",
    "df = pd.read_csv(dir)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dir0 = \"/homes/80/kang/cmem/output/06_r_output_raw_csv_fractional_shares/\"\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "onlyFiles = sorted([f for f in listdir(dir0) if isfile(join(dir0, f))])\n",
    "dfflst = []\n",
    "import pandas as pd\n",
    "i=0\n",
    "# for i in range(len(onlyFiles)):\n",
    "print(i)\n",
    "name = onlyFiles[i]\n",
    "symbol=name[:-4]\n",
    "import pandas as pd\n",
    "# name =\"CMS.csv\"\n",
    "\n",
    "dir = dir0+name\n",
    "df = pd.read_csv(dir)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "06_r_output_raw_csv_fractional_shares is more like the raw output of the r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# do the clipping inside the forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dir0 = \"/homes/80/kang/cmem/output/06_r_output_raw_csv_fractional_shares/\"\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "onlyFiles = sorted([f for f in listdir(dir0) if isfile(join(dir0, f))])\n",
    "dfflst = []\n",
    "import pandas as pd\n",
    "i=2\n",
    "# for i in range(len(onlyFiles)):\n",
    "print(i)\n",
    "name = onlyFiles[i]\n",
    "symbol=name[:-4]\n",
    "print(name)\n",
    "import pandas as pd\n",
    "# name =\"CMS.csv\"\n",
    "dir = dir0+name\n",
    "df = pd.read_csv(dir)\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "train_days,bin_size=10,26\n",
    "\n",
    "# def rolling_clip(df, factor):\n",
    "#     lookback_window = train_days*bin_size\n",
    "#     df[factor+\"_clipped\"]=df[factor]\n",
    "#     for i in range(train_days*bin_size,df.shape[0]):\n",
    "#         lookback_window_values = df.loc[:,factor].iloc[i-lookback_window:i]\n",
    "#         max_value=lookback_window_values.max()\n",
    "#         min_value=lookback_window_values.min()\n",
    "#         df.loc[:,factor+\"_clipped\"][i] = np.clip(df.loc[:,factor][i],min_value,max_value)\n",
    "#     return df\n",
    "\n",
    "def rolling_clip(df, factor):\n",
    "    lookback_window = train_days * bin_size\n",
    "    roll_min = df[factor].rolling(window=lookback_window + 1, min_periods=1).min().shift(1)\n",
    "    roll_max = df[factor].rolling(window=lookback_window + 1, min_periods=1).max().shift(1)\n",
    "    df[factor + \"_clipped\"] = df[factor].clip(lower=roll_min, upper=roll_max)\n",
    "    return df\n",
    "\n",
    "df = rolling_clip(df,\"daily\")\n",
    "df = rolling_clip(df,\"seasonal\")\n",
    "df = rolling_clip(df,\"dynamic\")\n",
    "df[\"forecast_signal_clipped\"] = df[\"daily_clipped\"]*df[\"seasonal_clipped\"]*df[\"dynamic_clipped\"]\n",
    "df[\"shares_forecast_signal_clipped\"] = df[\"forecast_signal_clipped\"]*df['sharesOut']\n",
    "df\n",
    "# df.to_csv(\"temp.csv\")\n",
    "# min(set(df.shares_r2))\n",
    "# df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## re-calc the r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.set_index(\"Unnamed: 0\")\n",
    "from sklearn.metrics import r2_score\n",
    "lst = []\n",
    "g=df.groupby(\"date\")\n",
    "for idx, itm in g:\n",
    "    r2=r2_score(itm[\"shares_original\"],itm[\"shares_forecast_signal_clipped\"])\n",
    "    lst.append([idx,r2])\n",
    "dff=pd.DataFrame(lst,columns=[\"date\",\"shares_r2_clipped\"])\n",
    "# dff['date'] = pd.to_datetime(dff['date'], format='%Y-%m.%d')\n",
    "df.shares_r2.unique().mean(),dff.shares_r2_clipped.unique().mean()\n",
    "# df.shares_r2.unique().min(),dff.shares_r2_clipped.unique().min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save to dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir /homes/80/kang/cmem/output/06_r_output_raw_csv_fractional_shares_clipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dir0 = \"/homes/80/kang/cmem/output/06_r_output_raw_csv_fractional_shares/\"\n",
    "dir1 = \"/homes/80/kang/cmem/output/06_r_output_raw_csv_fractional_shares_clipped/\"\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "onlyFiles = sorted([f for f in listdir(dir0) if isfile(join(dir0, f))])\n",
    "dfflst = []\n",
    "import pandas as pd\n",
    "i=2\n",
    "for i in range(len(onlyFiles)):\n",
    "    print(i)\n",
    "    name = onlyFiles[i]\n",
    "    symbol=name[:-4]\n",
    "    # print(name)\n",
    "    import pandas as pd\n",
    "    # name =\"CMS.csv\"\n",
    "    dir = dir0+name\n",
    "    df = pd.read_csv(dir)\n",
    "    import pandas as pd\n",
    "    train_days,bin_size=10,26\n",
    "    def rolling_clip(df, factor):\n",
    "        lookback_window = train_days * bin_size\n",
    "        roll_min = df[factor].rolling(window=lookback_window + 1, min_periods=1).min().shift(1)\n",
    "        roll_max = df[factor].rolling(window=lookback_window + 1, min_periods=1).max().shift(1)\n",
    "        df[factor + \"_clipped\"] = df[factor].clip(lower=roll_min, upper=roll_max)\n",
    "        return df\n",
    "\n",
    "    df = rolling_clip(df,\"daily\")\n",
    "    df = rolling_clip(df,\"seasonal\")\n",
    "    df = rolling_clip(df,\"dynamic\")\n",
    "    df[\"forecast_signal_clipped\"] = df[\"daily_clipped\"]*df[\"seasonal_clipped\"]*df[\"dynamic_clipped\"]\n",
    "    df[\"shares_forecast_signal_clipped\"] = df[\"forecast_signal_clipped\"]*df['sharesOut']\n",
    "    df = df.set_index(\"Unnamed: 0\")\n",
    "    from sklearn.metrics import r2_score\n",
    "    lst = []\n",
    "    g=df.groupby(\"date\")\n",
    "    for idx, itm in g:\n",
    "        r2=r2_score(itm[\"shares_original\"],itm[\"shares_forecast_signal_clipped\"])\n",
    "        lst.append([idx,r2])\n",
    "    dff=pd.DataFrame(lst,columns=[\"date\",\"shares_r2_clipped\"])\n",
    "    merged_df = pd.merge(df, dff, on='date')\n",
    "    merged_df.to_csv(dir1+symbol+\".csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read and merge for r2df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dir0 = \"/homes/80/kang/cmem/output/06_r_output_raw_csv_fractional_shares_clipped/\"\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "onlyFiles = sorted([f for f in listdir(dir0) if isfile(join(dir0, f))])\n",
    "dfflst = []\n",
    "import pandas as pd\n",
    "# i=2\n",
    "lst=[]\n",
    "for i in range(len(onlyFiles)):\n",
    "    print(i)\n",
    "    name = onlyFiles[i]\n",
    "    symbol=name[:-4]\n",
    "    # print(name)\n",
    "    import pandas as pd\n",
    "    # name =\"CMS.csv\"\n",
    "    dir = dir0+name\n",
    "    df = pd.read_csv(dir)\n",
    "    df['symbol']=symbol\n",
    "    f=df[['symbol','date','bins','original','shares_forecast_signal_clipped','shares_r2_clipped']]\n",
    "    lst.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h=pd.concat(lst)\n",
    "l=h[['symbol','date','shares_r2_clipped']]\n",
    "pivot_df = l.pivot_table(index='date', columns='symbol', values='shares_r2_clipped', aggfunc='mean')\n",
    "pivot_df.mean(axis=0),pivot_df.mean(axis=1),pivot_df.mean(axis=0).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load the original cmem data for r2df mean comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "dir0 = \"/homes/80/kang/cmem/output/06_r_output_raw_csv/\"\n",
    "onlyFiles = sorted([f for f in listdir(dir0) if isfile(join(dir0, f))])\n",
    "dfflst = []\n",
    "import pandas as pd\n",
    "i=0\n",
    "for i in range(len(onlyFiles)):\n",
    "    print(i)\n",
    "    name = onlyFiles[i]\n",
    "    import pandas as pd\n",
    "    # name =\"CMS.csv\"\n",
    "\n",
    "    dir = dir0+name\n",
    "    df = pd.read_csv(dir)\n",
    "    df.date = df.date.apply(lambda x:str(int(x)))\n",
    "    from sklearn.metrics import r2_score\n",
    "    lst = []\n",
    "    g=df.groupby(\"date\")\n",
    "    for idx, itm in g:\n",
    "        pass\n",
    "        r2=r2_score(itm[\"turnover\"],itm[\"x\"])\n",
    "        # r2=r2_score(itm[\"original\"],itm[\"forecast_signal\"])\n",
    "        lst.append([idx,name[:-4],r2])\n",
    "    dff=pd.DataFrame(lst,columns=[\"date\",\"symbol\",\"r2\"])\n",
    "    dfflst.append(dff)\n",
    "\n",
    "f=pd.concat(dfflst)\n",
    "h=f.pivot(index=\"date\",columns=\"symbol\",values=\"r2\")\n",
    "l=h.copy()\n",
    "l['symbol_mean']=l.mean(axis=1)\n",
    "l.loc['date_mean']=l.mean(axis=0)\n",
    "# l.to_csv(\"/homes/80/kang/cmem/0400_r_kl_output_qty_r2.csv\")\n",
    "h.mean(axis=1).mean()\n",
    "m=h.loc[:,pivot_df.columns]\n",
    "m.mean(axis=1).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test the assumption of the compressed big value hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min max \n",
    "log _ volume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test the result of combined lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  load data from 06_r_output_raw_csv_fractional_shares_clipped/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dir0 = \"/homes/80/kang/cmem/output/06_r_output_raw_csv_fractional_shares_clipped/\"\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "onlyFiles = sorted([f for f in listdir(dir0) if isfile(join(dir0, f))])\n",
    "dir1 = \"/homes/80/kang/cmem/output/06_r_output_raw_csv/\"\n",
    "dfflst = []\n",
    "import pandas as pd\n",
    "i=2\n",
    "# lst=[]\n",
    "# for i in range(len(onlyFiles)):\n",
    "print(i)\n",
    "name = onlyFiles[i]\n",
    "symbol=name[:-4]\n",
    "# print(name)\n",
    "import pandas as pd\n",
    "# name =\"CMS.csv\"\n",
    "dir = dir0+name\n",
    "df = pd.read_csv(dir)\n",
    "df['symbol']=symbol\n",
    "df[\"shares_daily_clipped\"]=df['daily_clipped']*df['sharesOut']\n",
    "f=df[['symbol','date','bins','original','shares_forecast_signal_clipped','shares_daily_clipped', 'seasonal_clipped',\n",
    "       'dynamic_clipped']]\n",
    "# lst.append(f)\n",
    "dff = pd.read_csv(dir1+name,index_col=0)\n",
    "dff['date'] = pd.to_datetime(dff['date'], format='%Y%m%d')\n",
    "\n",
    "f['x']=f['shares_forecast_signal_clipped']\n",
    "f['eta']=f['shares_daily_clipped']\n",
    "f['seas']=f['seasonal_clipped']\n",
    "f['mu']=f['dynamic_clipped']\n",
    "f['eta*seas']=f['eta']*f['seas']\n",
    "import numpy as np\n",
    "f['log_x']=f['x'].apply(np.log)\n",
    "f['log_eta*seas']=f['eta*seas'].apply(np.log)\n",
    "f['log_eta']=f['eta'].apply(np.log)\n",
    "f['log_seas']=f['seas'].apply(np.log)\n",
    "f['log_mu']=f['mu'].apply(np.log)\n",
    "cmem=[\n",
    " 'log_x',\n",
    " 'log_eta*seas',\n",
    " 'log_eta',\n",
    " 'log_seas',\n",
    " 'log_mu',\n",
    " 'x',\n",
    " 'eta*seas',\n",
    " 'eta',\n",
    " 'seas',\n",
    " 'mu'\n",
    " ]\n",
    "h=f[[\"date\"]+cmem]\n",
    "h['date'] = pd.to_datetime(h['date'])\n",
    "new_dff = dff.drop(columns=cmem)\n",
    "\n",
    "new_dff = new_dff.set_index('date')\n",
    "h = h.set_index('date')\n",
    "common_dates = new_dff.index.intersection(h.index)\n",
    "new_dff = new_dff.loc[common_dates]\n",
    "h = h.loc[common_dates]\n",
    "\n",
    "# merged = pd.merge(h, new_dff, on='date', how='left')\n",
    "n = pd.concat([h,new_dff],axis=1)\n",
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In [18]: df.columns\n",
    "Out[18]: \n",
    "Index(['Unnamed: 0', 'original', 'forecast_signal', 'daily', 'seasonal',\n",
    "       'dynamic', 'bins', 'date', 'r2', 'sharesOut', 'shares_forecast_signal',\n",
    "       'shares_original', 'shares_r2', 'daily_clipped', 'seasonal_clipped',\n",
    "       'dynamic_clipped', 'forecast_signal_clipped',\n",
    "       'shares_forecast_signal_clipped', 'shares_r2_clipped', 'symbol'],\n",
    "      dtype='object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff[[\n",
    " 'log_x',\n",
    " 'log_eta*seas',\n",
    " 'log_eta',\n",
    " 'log_seas',\n",
    " 'log_mu',\n",
    " 'x',\n",
    " 'eta*seas',\n",
    " 'eta',\n",
    " 'seas',\n",
    " 'mu'\n",
    " ]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save n to new_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir /homes/80/kang/cmem/output/0600_r_output_with_features_csv_fractional_shares_clipped/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = '/homes/80/kang/cmem/output/0600_r_output_with_features_csv_fractional_shares_clipped/'\n",
    "\n",
    "dir0 = \"/homes/80/kang/cmem/output/06_r_output_raw_csv_fractional_shares_clipped/\"\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "onlyFiles = sorted([f for f in listdir(dir0) if isfile(join(dir0, f))])\n",
    "dir1 = \"/homes/80/kang/cmem/output/06_r_output_raw_csv/\"\n",
    "dfflst = []\n",
    "import pandas as pd\n",
    "for i in range(len(onlyFiles)):\n",
    "    print(i)\n",
    "    name = onlyFiles[i]\n",
    "    symbol=name[:-4]\n",
    "    # print(name)\n",
    "    import pandas as pd\n",
    "    # name =\"CMS.csv\"\n",
    "    dir = dir0+name\n",
    "    df = pd.read_csv(dir)\n",
    "    df['symbol']=symbol\n",
    "    df[\"shares_daily_clipped\"]=df['daily_clipped']*df['sharesOut']\n",
    "    f=df[['symbol','date','bins','original','shares_forecast_signal_clipped','shares_daily_clipped', 'seasonal_clipped',\n",
    "        'dynamic_clipped']]\n",
    "    # lst.append(f)\n",
    "    dff = pd.read_csv(dir1+name,index_col=0)\n",
    "    dff['date'] = pd.to_datetime(dff['date'], format='%Y%m%d')\n",
    "\n",
    "    f['x']=f['shares_forecast_signal_clipped']\n",
    "    f['eta']=f['shares_daily_clipped']\n",
    "    f['seas']=f['seasonal_clipped']\n",
    "    f['mu']=f['dynamic_clipped']\n",
    "    f['eta*seas']=f['eta']*f['seas']\n",
    "    import numpy as np\n",
    "    f['log_x']=f['x'].apply(np.log)\n",
    "    f['log_eta*seas']=f['eta*seas'].apply(np.log)\n",
    "    f['log_eta']=f['eta'].apply(np.log)\n",
    "    f['log_seas']=f['seas'].apply(np.log)\n",
    "    f['log_mu']=f['mu'].apply(np.log)\n",
    "    cmem=[\n",
    "    'log_x',\n",
    "    'log_eta*seas',\n",
    "    'log_eta',\n",
    "    'log_seas',\n",
    "    'log_mu',\n",
    "    'x',\n",
    "    'eta*seas',\n",
    "    'eta',\n",
    "    'seas',\n",
    "    'mu'\n",
    "    ]\n",
    "    h=f[[\"date\"]+cmem]\n",
    "    h['date'] = pd.to_datetime(h['date'])\n",
    "    new_dff = dff.drop(columns=cmem)\n",
    "\n",
    "    new_dff = new_dff.set_index('date')\n",
    "    h = h.set_index('date')\n",
    "    common_dates = new_dff.index.intersection(h.index)\n",
    "    new_dff = new_dff.loc[common_dates]\n",
    "    h = h.loc[common_dates]\n",
    "\n",
    "    # merged = pd.merge(h, new_dff, on='date', how='left')\n",
    "    n = pd.concat([h,new_dff],axis=1)\n",
    "    n.to_csv(output_dir+name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## next is to run the lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data from 0600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "dir0 = \"/homes/80/kang/cmem/output/06_r_output_raw_csv/\"\n",
    "onlyFiles = sorted([f for f in listdir(dir0) if isfile(join(dir0, f))])\n",
    "dfflst = []\n",
    "import pandas as pd\n",
    "i=0\n",
    "# for i in range(len(onlyFiles)):\n",
    "print(i)\n",
    "name = onlyFiles[i]\n",
    "import pandas as pd\n",
    "# name =\"CMS.csv\"\n",
    "\n",
    "dir = dir0+name\n",
    "df = pd.read_csv(dir,index_col=0)\n",
    "df\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "x",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
